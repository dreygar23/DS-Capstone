{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os \n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "import datetime\n",
    "from pandasql import sqldf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model Building\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.utils import resample\n",
    "from scipy.special import softmax\n",
    "import plot_utils as easyplt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to help calculate the pass/fail status of target course\n",
    "def pf_flag(grade): \n",
    "    if grade in ['A', 'A-', 'B', 'B+', 'B-', 'C', 'C+', 'P']: \n",
    "        return '-1'\n",
    "    return '1'\n",
    "\n",
    "\n",
    "# Creates new columns and a unions dataframes to prep for model input\n",
    "def union_data(train, valid, test):\n",
    "    # Create new columns per dataframe\n",
    "    train['pf_flag'] = train.apply(lambda x: 1 if (x['Pass_Fail'] == 'pass') else -1, axis = 1) \n",
    "    train['source'] = 'train'\n",
    "    valid['pf_flag'] = valid.apply(lambda x: 1 if (x['Pass_Fail'] == 'pass') else -1, axis = 1) \n",
    "    valid['source'] = 'validate'\n",
    "    test['pf_flag'] = test.apply(lambda x: 1 if (x['Pass_Fail'] == 'pass') else -1, axis = 1) \n",
    "    test['source'] = 'test'\n",
    "    \n",
    "    # Union all dataframes\n",
    "    dfs = [train, valid, test]\n",
    "    union = pd.concat(dfs)\n",
    "    \n",
    "    return union\n",
    "\n",
    "# Identify the courses with highest enrollment (>= 20% of students enrolled)\n",
    "def id_common_crs(train_df):\n",
    "    #Create a grouping by course to identify high enrollment courses\n",
    "    crs_by_enrl = train_df.groupby(['unique_course']).agg({'uuid': ['nunique']})\n",
    "    crs_by_enrl.columns = crs_by_enrl.columns.droplevel(0)\n",
    "    crs_by_enrl = crs_by_enrl.reset_index(inplace=False)\n",
    "    crs_by_enrl.sort_values(by=['nunique'], inplace=True, ascending=False)\n",
    "    \n",
    "    #Create new columns\n",
    "    crs_by_enrl['tot_stu'] = train_df.uuid.nunique()\n",
    "    crs_by_enrl['perc'] = (crs_by_enrl['nunique'] / crs_by_enrl['tot_stu']) * 100\n",
    "    \n",
    "    #Calculate percent of total to use to create 20% threshhold \n",
    "    crs_data = crs_by_enrl.loc[crs_by_enrl['perc'] >= 20].copy()\n",
    "    crs_data.reset_index(inplace=True)\n",
    "    \n",
    "    #List of common courses\n",
    "    common_crs = crs_data['unique_course'].tolist()\n",
    "    \n",
    "    return common_crs\n",
    "\n",
    "# Function imports file and creates df; drops unnecessary columns and converts to correct data types\n",
    "# Function also returns list of common courses taken from whole dataset\n",
    "def import_file(file_path):\n",
    "    \n",
    "    #Move directories to find file\n",
    "    os.getcwd()\n",
    "    os.chdir('../data')\n",
    "\n",
    "    \n",
    "    #Read excel file and drop unnecessary column\n",
    "    wkbk = pd.ExcelFile(file_path)\n",
    "    #train\n",
    "    t1 = pd.read_excel(wkbk, 'train')  \n",
    "    #validate\n",
    "    v = pd.read_excel(wkbk, 'validate') \n",
    "    #test\n",
    "    t2 = pd.read_excel(wkbk, 'test')     \n",
    "    \n",
    "    # Change data type of enrl_term_id\n",
    "    t1['enrl_term_id'] = t1['enrl_term_id'].astype(int)\n",
    "    v['enrl_term_id'] = t1['enrl_term_id'].astype(int)\n",
    "    t2['enrl_term_id'] = t1['enrl_term_id'].astype(int)\n",
    "    \n",
    "    # Create pf_flag for unique_courses\n",
    "    t1['pf_flag'] = t1.apply(lambda x: -1 if (x['Pass_Fail'] == 'pass') else 1, axis = 1) \n",
    "    v['pf_flag'] = v.apply(lambda x: -1 if (x['Pass_Fail'] == 'pass') else 1, axis = 1) \n",
    "    t2['pf_flag'] = t2.apply(lambda x: -1 if (x['Pass_Fail'] == 'pass') else 1, axis = 1) \n",
    "\n",
    "    # Create target_crs_pass_fail for target course\n",
    "    t1['target_crs_pass_fail'] = t1['target_crs_grade'].apply(pf_flag)\n",
    "    v['target_crs_pass_fail'] = v['target_crs_grade'].apply(pf_flag)\n",
    "    t2['target_crs_pass_fail'] = t2['target_crs_grade'].apply(pf_flag)\n",
    "    \n",
    "    \n",
    "    # Create percentage flags for UCCs and Gateways\n",
    "    t1['perc_ucc_passed'] = t1['UCCs_passed'] / t1['UCCs_taken']\n",
    "    t1['perc_gateway_passed'] = t1['gateways_passed'] / t1['gateways_taken']\n",
    "    t1['perc_ucc_failed'] = t1['UCCs_failed'] / t1['UCCs_taken']\n",
    "    t1['perc_gateway_failed'] = t1['gateways_failed'] / t1['gateways_taken']\n",
    "    \n",
    "    v['perc_ucc_passed'] = v['UCCs_passed'] / v['UCCs_taken']\n",
    "    v['perc_gateway_passed'] = v['gateways_passed'] / v['gateways_taken']\n",
    "    v['perc_ucc_failed'] = v['UCCs_failed'] / v['UCCs_taken']\n",
    "    v['perc_gateway_failed'] = v['gateways_failed'] / v['gateways_taken']\n",
    "    \n",
    "    t2['perc_ucc_passed'] = t2['UCCs_passed'] / t2['UCCs_taken']\n",
    "    t2['perc_gateway_passed'] = t2['gateways_passed'] / t2['gateways_taken']\n",
    "    t2['perc_ucc_failed'] = t2['UCCs_failed'] / t2['UCCs_taken']\n",
    "    t2['perc_gateway_failed'] = t2['gateways_failed'] / t2['gateways_taken']\n",
    "    \n",
    "    \n",
    "    # Drop Pass_Fail flag\n",
    "    t1.drop(['Pass_Fail','target_crs_grade'], axis=1, inplace = True)\n",
    "    v.drop(['Pass_Fail','target_crs_grade'], axis=1, inplace = True)\n",
    "    t2.drop(['Pass_Fail','target_crs_grade'], axis=1, inplace = True)\n",
    "    \n",
    "    # Find common courses students take and create list (from training data only)\n",
    "    crs_ = id_common_crs(t1)\n",
    "    \n",
    "    \n",
    "    return t1, v, t2, crs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate Data and prep for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify object vs numerical columns\n",
    "def identify_cols(df):\n",
    "    dtypes = df.dtypes.to_dict()\n",
    "    obj_list = []\n",
    "    num_list = []\n",
    "    exclude_obj = ['uuid', 'unique_course', 'target_crs_pass_fail', 'target_crs']\n",
    "    exclude_num = ['pf_flag', 'target_crs_pass_fail']\n",
    "    \n",
    "    for col_name, typ in dtypes.items():\n",
    "        if (typ == 'O'and col_name not in exclude_obj ): \n",
    "            obj_list.append(col_name)\n",
    "            \n",
    "        elif (typ != 'O' and typ != 'bool' and col_name not in exclude_num):\n",
    "            num_list.append(col_name)\n",
    "            \n",
    "    return obj_list, num_list\n",
    "\n",
    "\n",
    "# One hot encode train, validation and test data\n",
    "def one_hot_encode(train_data, valid_data, test_data, obj_list):\n",
    "    \n",
    "    # Keep copy of all other data\n",
    "    train_other = train_data.drop(obj_list, axis=1, inplace=False).copy()\n",
    "    valid_other = valid_data.drop(obj_list, axis=1, inplace=False).copy()\n",
    "    test_other = test_data.drop(obj_list, axis=1, inplace=False).copy()\n",
    "    \n",
    "    \n",
    "    # Create one hot encoder object\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown ='ignore')\n",
    "    \n",
    "    # Encode train, validation and test data\n",
    "    train_hot_encoded = ohe.fit_transform(train_data[obj_list])\n",
    "    valid_hot_encoded = ohe.transform(valid_data[obj_list])\n",
    "    test_hot_encoded = ohe.transform(test_data[obj_list])\n",
    " \n",
    "    # Create dfs to return encoded data\n",
    "    train_df_ = pd.DataFrame(train_hot_encoded)\n",
    "    valid_df_ = pd.DataFrame(valid_hot_encoded)\n",
    "    test_df_ = pd.DataFrame(test_hot_encoded)\n",
    "    \n",
    "    # Add column names\n",
    "    train_df_.columns = ohe.get_feature_names_out()\n",
    "    valid_df_.columns = ohe.get_feature_names_out()\n",
    "    test_df_.columns = ohe.get_feature_names_out()\n",
    "    \n",
    "    # Add uuids back\n",
    "    train_df = pd.concat([train_other, train_df_], axis=1)\n",
    "    valid_df = pd.concat([valid_other, valid_df_], axis=1)\n",
    "    test_df = pd.concat([test_other, test_df_], axis=1)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "# Scale numerical data    \n",
    "def scale_data(train_data, valid_data, test_data, num_cols_list):    \n",
    "    \n",
    "    # Sort data using enrollment term; latest to oldest\n",
    "    train_data.sort_values(by=['enrl_term_id'], inplace=True, ascending=False)\n",
    "    valid_data.sort_values(by=['enrl_term_id'], inplace=True, ascending=False)\n",
    "    test_data.sort_values(by=['enrl_term_id'], inplace=True, ascending=False)\n",
    "    \n",
    "    '''TRAIN DATA FIRST'''\n",
    "    # Extract only the numerical columns \n",
    "    train_num_cols = train_data[num_cols_list].copy()\n",
    "    # Rest of columns\n",
    "    train_other = train_data.drop(num_cols_list, axis=1, inplace=False).copy()\n",
    "    train_other.reset_index(inplace=True, drop=True)\n",
    "    # Scale using train numerical data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = pd.DataFrame(scaler.fit_transform(train_num_cols), \n",
    "                  columns=num_cols_list)\n",
    "    # Concatenate all train data together again\n",
    "    train_all = pd.concat([train_other, train_scaled], axis=1)\n",
    "    # Reset indexes\n",
    "    train_all.reset_index(inplace=True, drop=True)\n",
    " \n",
    "\n",
    "    '''VALIDATION DATA SECOND'''\n",
    "    # Extract only the numerical columns \n",
    "    valid_num_cols = valid_data[num_cols_list].copy()\n",
    "    # Rest of columns\n",
    "    valid_other = valid_data.drop(num_cols_list, axis=1, inplace=False).copy()\n",
    "    valid_other.reset_index(inplace=True, drop=True)\n",
    "    # Scale using previously created scaler (using train numerical data)\n",
    "    valid_scaled = pd.DataFrame(scaler.transform(valid_num_cols), \n",
    "                  columns=num_cols_list)\n",
    "    \n",
    "    valid_all = pd.concat([valid_other, valid_scaled], axis=1)\n",
    "    # Reset indexes\n",
    "    valid_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    \n",
    "    '''TEST DATA LAST'''\n",
    "    # Extract only the numerical columns \n",
    "    test_num_cols = test_data[num_cols_list].copy()\n",
    "    # Rest of columns\n",
    "    test_other = test_data.drop(num_cols_list, axis=1, inplace=False).copy()\n",
    "    test_other.reset_index(inplace=True, drop=True)\n",
    "    # Scale using previously created scaler (using train numerical data)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test_num_cols), \n",
    "                  columns=num_cols_list)\n",
    "    \n",
    "    test_all = pd.concat([test_other, test_scaled], axis=1)\n",
    "    # Reset indexes\n",
    "    test_all.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "\n",
    "    '''ALL DATA - CHANGES'''\n",
    "    \n",
    "    # Remove enrl_term_id\n",
    "    train_all.drop('enrl_term_id', axis=1, inplace=True)\n",
    "    valid_all.drop('enrl_term_id', axis=1, inplace=True)\n",
    "    test_all.drop('enrl_term_id', axis=1, inplace=True)\n",
    "    \n",
    "    return train_all, valid_all, test_all\n",
    "\n",
    "\n",
    "# Data pivot of unique_course for most common courses only\n",
    "def pivot_courses(df, crs_list):\n",
    "     \n",
    "    # Create df containing only those courses of interest (prev identified above threshhold enrollment #s)\n",
    "    df_filtered = df.loc[df['unique_course'].isin(crs_list)].copy()\n",
    "    \n",
    "    # Create df that does not contain unique_course column\n",
    "    df_nocrs = df.loc[:, df.columns!='unique_course']\n",
    "    \n",
    "    # Create pivot table to use as input and fill NA with 0\n",
    "    df_pivot = df_filtered.pivot_table(index='uuid', \n",
    "                                values='pf_flag', columns='unique_course').reset_index().rename_axis(None, axis=1)\n",
    "    df_pivot.fillna(0, inplace = True)\n",
    "    \n",
    "    # Merge pivoted courses with rest of data\n",
    "    new_data = pd.merge(df_nocrs, df_pivot, how='left', on=['uuid'],\n",
    "         suffixes=('_og', '_stu'), copy=True, indicator=False,\n",
    "         validate=None)\n",
    "\n",
    "    new_data.fillna(0, inplace = True)\n",
    "    \n",
    "    new_data.drop(['target_crs', 'pf_flag'], axis=1, inplace=True)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "# Check to ensure that test data looks like train data after course pivot\n",
    "def check_data_shape(train_df, test_df):\n",
    "    \n",
    "    # Add columns to test data if they existed in train data to maintain shape\n",
    "    for col in train_df.columns:\n",
    "        if col not in test_df.columns:\n",
    "            test_df[col] = 0\n",
    "            \n",
    "            \n",
    "    return test_df\n",
    "\n",
    "\n",
    "# Ensure that target variable is the last variable\n",
    "def make_target_last(df):\n",
    "    last = df['target_crs_pass_fail'].copy()\n",
    "    other = df.drop('target_crs_pass_fail', axis=1, inplace=False)\n",
    "    new_df = pd.concat([other, last], 1)\n",
    "    \n",
    "    new_df.drop_duplicates(subset=\"uuid\", keep ='first', inplace=True)\n",
    "\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "# Creates X (attribute) and Y (target) arrays for data\n",
    "def create_arrays(df):\n",
    "    \n",
    "    if 'uuid' in df.columns:\n",
    "        df.drop('uuid', axis=1, inplace=True)\n",
    "        \n",
    "    # Create separate df\n",
    "    attributes = df.drop('target_crs_pass_fail', axis=1).copy()\n",
    "    target = df['target_crs_pass_fail'].copy()\n",
    "    \n",
    "    # Create arrays to return\n",
    "    X_ = attributes.values\n",
    "    y_ = target.values\n",
    "    \n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy classifier\n",
    "def run_stratefied_dummy(X, y):\n",
    "    \n",
    "    averages = {}\n",
    "    accuracy = []\n",
    "    f1_macro = []\n",
    "    f1_pos = []\n",
    "    f1_neg = []\n",
    "    precision =[]\n",
    "    recall = []\n",
    "\n",
    "\n",
    "    all_scores = {}\n",
    "    \n",
    "    for file in file_names:\n",
    "        \n",
    "        strat = DummyClassifier(strategy=\"stratified\", random_state = rng)\n",
    "        strat.fit(X[file], y[file])\n",
    "        strat_pred = strat.predict(X[file])\n",
    "        #strat_score = strat.score(X, y)\n",
    "        f1_labels = f1_score(y[file], strat_pred, zero_division=0, average=None)\n",
    "        \n",
    "        accuracy.append(accuracy_score(y[file], strat_pred))\n",
    "        f1_macro.append(f1_score(y[file], strat_pred, average='macro'))\n",
    "        f1_pos.append(f1_labels[1])\n",
    "        f1_neg.append(f1_labels[0])\n",
    "        precision.append(precision_score(y[file], strat_pred, pos_label='1', zero_division=1))\n",
    "        recall.append(recall_score(y[file], strat_pred, pos_label='1', zero_division=1))\n",
    "        \n",
    "        all_scores[file] = {#'accuracy' : accuracy_score(y[file], strat_pred), \n",
    "                       'f1_positive': f1_labels[1], 'f1_negative': f1_labels[0],\n",
    "                       'f1_macro': f1_score(y[file], strat_pred, average='macro'),\n",
    "                        'precision': precision_score(y[file], strat_pred, pos_label='1', zero_division=1),\n",
    "                        'recall': recall_score(y[file], strat_pred, pos_label='1', zero_division=1)}\n",
    "        \n",
    "        auc = metrics.roc_auc_score(y[file].astype(int), strat_pred.astype(int))\n",
    "        print(file, \" \" , auc)\n",
    "    averages = {'f1_positive': statistics.mean(f1_pos), 'f1_negative': statistics.mean(f1_neg),\n",
    "                       'f1_macro': statistics.mean(f1_macro),\n",
    "                'precision': statistics.mean(precision),\n",
    "                'recall': statistics.mean(recall)}\n",
    "        \n",
    "        \n",
    "    return averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, Preprocess and Prepare Data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High enrollment courses list\n",
    "file_names = [ 'PSY3211', 'PSY3024', 'PSY3215', 'PSY4931', 'MAR3023', 'ENC1102', 'EAB3002', \n",
    "              'QMB3200','MAN3025', 'FIN3403', 'DEP3305', 'GEB3003', 'QMB4680',  \n",
    "              'CCJ4014', 'EXP3523', 'BUL4310', 'CCJ3628', 'MAN4720', 'SOP3004', 'CLP4146', \n",
    "              'BSC2023', 'CJL4064', 'CLP4374']\n",
    "\n",
    "\n",
    "# Import all files into dictionary\n",
    "def all_files():\n",
    "       \n",
    "    train_dict = {}\n",
    "    valid_dict = {}\n",
    "    test_dict = {}\n",
    "    \n",
    "    courses_list = {}\n",
    "    \n",
    "    for file in file_names:\n",
    "        path = 'files/{0}.xlsx'.format(file)\n",
    "        train, validate, test, common_crs = import_file(path)\n",
    "        \n",
    "        train_dict[file] = train\n",
    "        valid_dict[file] = validate\n",
    "        test_dict[file] = test\n",
    "        \n",
    "        courses_list[file] = common_crs\n",
    "        \n",
    "    return train_dict, valid_dict, test_dict, courses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "def pre_process_all(train_, valid_, test_, courses):  \n",
    "\n",
    "    train_pro = {}\n",
    "    valid_pro = {}\n",
    "    test_pro = {}\n",
    "    \n",
    "    for file in file_names:\n",
    "    \n",
    "        # Create obj and num lists\n",
    "        obj, num = identify_cols(train_[file])\n",
    "\n",
    "        # One hot encode data first: \n",
    "        tr_one, valid_one, test_one = one_hot_encode(train_[file], valid_[file], test_[file], obj)\n",
    "\n",
    "        # Scale numerical data: \n",
    "        tr_scale, valid_scale, test_scale = scale_data(tr_one, valid_one, test_one, num)\n",
    "\n",
    "        # Pivot courses next: \n",
    "        train_piv = pivot_courses(tr_scale, courses[file])\n",
    "        valid_piv = pivot_courses(valid_scale, courses[file])\n",
    "        test_piv = pivot_courses(test_scale, courses[file])\n",
    "\n",
    "        # Ensure that train, validation and test are the same shape\n",
    "        train_mid = train_piv.copy()\n",
    "        valid_mid = check_data_shape(train_piv, valid_piv)\n",
    "        test_mid = check_data_shape(train_piv, test_piv) \n",
    "\n",
    "        # Ensure that target is the last column\n",
    "        train_final = make_target_last(train_mid)\n",
    "        valid_final = make_target_last(valid_mid)\n",
    "        test_final = make_target_last(test_mid)\n",
    "        \n",
    "        # Add to dictionaries\n",
    "        train_pro[file] = train_final\n",
    "        valid_pro[file] = valid_final\n",
    "        test_pro[file] = test_final\n",
    "        \n",
    "        \n",
    "    return train_pro, valid_pro, test_pro\n",
    "    \n",
    "# Create datasets    \n",
    "def create_datasets(t, v, te, crs_list):\n",
    "    #Enrollment + courses\n",
    "    train_enrl = {}\n",
    "    valid_enrl = {}\n",
    "    test_enrl = {}\n",
    "    \n",
    "    #Courses only\n",
    "    train_courses = {}\n",
    "    valid_courses = {}\n",
    "    test_courses = {}\n",
    "    \n",
    "    \n",
    "    #ANOVA FEATURES\n",
    "    cols = [ 'UCCs_failed', 'total_courses_passed', 'target_avg_grade_all', 'UCCs_passed', 'term_gpa', 'perc_ucc_passed', \n",
    "            'creds_attp_term', 'total_courses_failed', 'target_avg_grd_term', 'perc_ucc_failed', 'perc_gateway_passed', \n",
    "            'perc_gateway_failed', 'INST_GPA']\n",
    "    \n",
    "    \n",
    "    train_vars = {}\n",
    "    valid_vars = {}\n",
    "    test_vars = {}\n",
    "    \n",
    "    for file in file_names:\n",
    "        \n",
    "        '''ENROLLMENT VARS ALL'''\n",
    "        # All enrollment vars (no demographics)\n",
    "        train_ecrs = t[file].iloc[:,list(range(12,len(t[file].columns)))]\n",
    "        valid_ecrs = v[file].iloc[:,list(range(12,len(v[file].columns)))]\n",
    "        test_ecrs = te[file].iloc[:,list(range(12,len(te[file].columns)))]\n",
    "        \n",
    "        # Add to dictionaries\n",
    "        train_enrl[file] = train_ecrs\n",
    "        valid_enrl[file] = valid_ecrs\n",
    "        test_enrl[file] = test_ecrs\n",
    "        \n",
    "        '''COURSE VARS ONLY'''\n",
    "        # All enrollment vars (no demographics)\n",
    "        train_crs = t[file].iloc[:,list(range((len(t[file].columns) - len(crs_list[file])),len(t[file].columns)))]\n",
    "        valid_crs = v[file].iloc[:,list(range((len(v[file].columns) - len(crs_list[file])),len(v[file].columns)))]\n",
    "        test_crs = te[file].iloc[:,list(range((len(te[file].columns) - len(crs_list[file])),len(te[file].columns)))]\n",
    "          \n",
    "        #Add to dictionaries\n",
    "        train_courses[file] = train_crs\n",
    "        valid_courses[file] = valid_crs\n",
    "        test_courses[file] = test_crs\n",
    "        \n",
    "        '''VAR SELECTION DATA'''\n",
    "        \n",
    "        # Selected variables\n",
    "        train_sel = t[file][cols].copy()\n",
    "        valid_sel = v[file][cols].copy()\n",
    "        test_sel = te[file][cols].copy()\n",
    "        \n",
    "        \n",
    "        # Concatenate dfs\n",
    "        train_sel_all = pd.concat([train_sel, train_crs], axis=1)\n",
    "        valid_sel_all = pd.concat([valid_sel, valid_crs], axis=1)\n",
    "        test_sel_all = pd.concat([test_sel, test_crs], axis=1)\n",
    "          \n",
    "        #Add to dictionaries\n",
    "        train_vars[file] = train_sel_all\n",
    "        valid_vars[file] = valid_sel_all\n",
    "        test_vars[file] = test_sel_all\n",
    "        \n",
    "    \n",
    "    return train_enrl, valid_enrl, test_enrl, train_courses, valid_courses, test_courses, train_vars, valid_vars, test_vars\n",
    "\n",
    "def create_arrays_all(t, v, te):\n",
    "    \n",
    "    X_train_arr = {}\n",
    "    y_train_arr = {}\n",
    "    X_valid_arr = {}\n",
    "    y_valid_arr = {}\n",
    "    X_test_arr = {}\n",
    "    y_test_arr = {}\n",
    "    \n",
    "    for file in file_names:\n",
    "        \n",
    "        # Prepare training data for model ingestion\n",
    "        X_train, y_train = create_arrays(t[file])\n",
    "        X_valid, y_valid = create_arrays(v[file])\n",
    "        X_test, y_test = create_arrays(te[file])\n",
    "        \n",
    "        # Add to dictionaries\n",
    "        X_train_arr[file] = X_train\n",
    "        y_train_arr[file] = y_train\n",
    "        X_valid_arr[file] = X_valid\n",
    "        y_valid_arr[file] = y_valid\n",
    "        X_test_arr[file] = X_test\n",
    "        y_test_arr[file] = y_test\n",
    "\n",
    "    return X_train_arr, y_train_arr, X_valid_arr, y_valid_arr, X_test_arr, y_test_arr\n",
    "\n",
    "\n",
    "def create_train_arrays_all(t):\n",
    "    \n",
    "    X_train_arr = {}\n",
    "    y_train_arr = {}\n",
    "    \n",
    "    for file in file_names:\n",
    "        \n",
    "        # Prepare training data for model ingestion\n",
    "        X_train, y_train = create_arrays(t[file])\n",
    "        \n",
    "        \n",
    "        # Add to dictionaries\n",
    "        X_train_arr[file] = X_train\n",
    "        y_train_arr[file] = y_train\n",
    "        \n",
    "\n",
    "    return X_train_arr, y_train_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sets to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, valid_, test_, courses = all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess files and create dictionaries with data ready for model ingestion\n",
    "train_p, valid_p, test_p = pre_process_all(train_, valid_, test_, courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enrollment variables only dataset as well as courses only dataset for comparison\n",
    "train_enrl_crs, valid_enrl_crs, test_enrl_crs, train_crs, valid_crs, test_crs, train_sel_var, valid_sel_var, test_sel_var = create_datasets(train_p, valid_p, test_p, courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for courses only\n",
    "X_train_crs, y_train_crs, X_valid_crs, y_valid_crs, X_test_crs, y_test_crs = create_arrays_all(train_crs, valid_crs, test_crs)\n",
    "\n",
    "# Create arrays for enrollment + courses\n",
    "X_train_enrl, y_train_enrl, X_valid_enrl, y_valid_enrl, X_test_enrl, y_test_enrl = create_arrays_all(train_enrl_crs, valid_enrl_crs, test_enrl_crs)\n",
    "\n",
    "# Create arrays for full data set\n",
    "#X_train_full, y_train_full, X_valid_full, y_valid_full, X_test_full, y_test_full = create_arrays_all(train_p, valid_p, test_p)\n",
    "\n",
    "# Create arrays for variables selected\n",
    "X_train_select, y_train_select, X_valid_select, y_valid_select, X_test_select, y_test_select = create_arrays_all(train_sel_var, valid_sel_var, test_sel_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking training/valid/test sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_course = 'CLP4374'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_crs_pass_fail\n",
       "-1    1383\n",
       "1       72\n",
       "Name: target_crs_pass_fail, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p[t_course].groupby('target_crs_pass_fail')['target_crs_pass_fail'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 70)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p[t_course].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run baselines - on enrollment + courses dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrollment + courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_baseline  = run_stratefied_dummy(X_train_enrl, y_train_enrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strat_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure to use predefined validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use with imbalanced data \n",
    "svc_params = {\n",
    "              'C': [1, 10, 100], \n",
    "              'gamma': [0.001],\n",
    "              'kernel': ['linear','rbf', 'poly'],\n",
    "              'class_weight': ['balanced'] }\n",
    "\n",
    "tree_params = {'criterion': [\"gini\"]\n",
    "              'splitter': [\"best\",\"random\"]\n",
    "              }\n",
    "\n",
    "ada_params = {'learning_rate': [0.01, 0.05, 0.1],\n",
    "              'algorithm': ['SAMME', 'SAMME.R'],\n",
    "              'n_estimators' : [50,100]}\n",
    "    \n",
    "\n",
    "\n",
    "# For use with pipelines\n",
    "svc_params_adj = {'svc__' + key: svc_params[key] for key in svc_params}\n",
    "tree_params_adj = {'decisiontreeclassifier__' + key: tree_params[key] for key in tree_params}\n",
    "ada_params_adj = {'adaboostclassifier__' + key: ada_params[key] for key in ada_params}\n",
    "\n",
    "# Function runs grid search on single course after performing SMOTE on the data set to deal with class imbalances\n",
    "def run_grid_search_SMOTE(model, params, X_train, y_train, X_valid, y_valid, file):\n",
    "    \n",
    "    '''Cross Validation Predefined Split'''\n",
    "    X = np.vstack((X_train, X_valid))\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_valid.shape[0])]\n",
    "    y = np.concatenate([y_train, y_valid])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    '''Create Pipeline'''\n",
    "    imba_pipeline = make_pipeline(SMOTE(random_state=1), \n",
    "                                  model)\n",
    "    \n",
    "    '''Create scorer object'''\n",
    "    def adjusted_f1(y_true, y_prob):\n",
    "        f1 = f1_score(y_true, y_prob, zero_division=0, average = 'binary', pos_label='1')\n",
    "        return f1\n",
    "    \n",
    "    score = make_scorer(adjusted_f1, greater_is_better = True)\n",
    "    \n",
    "    \n",
    "    '''Implementing Gridsearch'''\n",
    "    grid_imba = GridSearchCV(imba_pipeline, param_grid=params, cv=ps, scoring=score, refit=True,\n",
    "                        return_train_score=True)\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    grid_imba.fit(X, y)\n",
    "    \n",
    "    mean_train = statistics.mean(grid_imba.cv_results_['mean_train_score'])\n",
    "    mean_test = statistics.mean(grid_imba.cv_results_['mean_test_score'])\n",
    "    \n",
    "   \n",
    "    '''Feature Selection'''\n",
    "    \n",
    "    rfe = RFECV(grid_imba.best_estimator_.named_steps[\"adaboostclassifier\"], step=1, verbose=1, cv=ps, min_features_to_select=10) \n",
    "    rfe_selector = rfe.fit(X,y)\n",
    "    print(rfe_selector.ranking_)\n",
    "    print(rfe_selector.support_)\n",
    "    print(rfe_selector.n_features_)\n",
    "    \n",
    "    rfe_results = {'rank': rfe_selector.ranking_, 'supp': rfe_selector.support_, 'feat': rfe_selector.n_features_}\n",
    "    \n",
    "    return grid_imba.best_params_, mean_train, mean_test, grid_imba.best_estimator_ , rfe_results \n",
    "\n",
    "\n",
    "# Function runs grid search on all courses using SMOTE to deal with class imbalances\n",
    "def gridsearch_all_SMOTE(model_chosen, params, X_train, y_train, X_valid, y_valid):\n",
    "\n",
    "    all_results = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    estimator = {}\n",
    "    cv_results_all = {}\n",
    "    rfe = {}\n",
    "\n",
    "    \n",
    "    for file in file_names:\n",
    "    \n",
    "        m_results, m_train_score, m_test_score, m_estimator , rfe_res= run_grid_search_SMOTE(model_chosen, params, \n",
    "                                                    X_train[file], y_train[file], X_valid[file], y_valid[file], file)\n",
    "\n",
    "\n",
    "\n",
    "        all_results.append(m_results)\n",
    "        train_scores.append(m_train_score)\n",
    "        test_scores.append(m_test_score)\n",
    "        estimator[file] = m_estimator\n",
    "        rfe[file] = rfe_res\n",
    "    \n",
    "        \n",
    "    return all_results, train_scores, test_scores, estimator, rfe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABOOST WITH CRS FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=1, random_state=1)\n",
    "results00, train_scores00, test_scores00, models00 = gridsearch_all_SMOTE(AdaBoostClassifier(base_estimator = estimator, random_state = 1), ada_params_adj, \n",
    "                                                        X_train_crs,y_train_crs,X_valid_crs,y_valid_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABOOST WITH ENRL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=1, random_state=1)\n",
    "results0, train_scores0, test_scores0, models0 = gridsearch_all_SMOTE(AdaBoostClassifier(base_estimator = estimator, random_state = 1), ada_params_adj, \n",
    "                                                        X_train_enrl,y_train_enrl,X_valid_enrl,y_valid_enrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABOOST WITH SELECTED ANOVA FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=1, random_state=1)\n",
    "results11, train_scores11, test_scores11, models11 = gridsearch_all_SMOTE(AdaBoostClassifier(base_estimator = estimator, random_state = 1), ada_params_adj, \n",
    "                                                        X_train_select,y_train_select,X_valid_select,y_valid_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM WITH ALL ENROLLMENT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results2, train_scores2, test_scores2, models2 = gridsearch_all_SMOTE(SVC(random_state=1, probability=True),svc_params_adj, \n",
    "                                          X_train_enrl,y_train_enrl,X_valid_enrl,y_valid_enrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM WITH SELECTED ANOVA FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results31, train_scores31, test_scores31, models31 = gridsearch_all_SMOTE(SVC(random_state=1, probability=True),svc_params_adj, \n",
    "                                          X_train_select,y_train_select,X_valid_select,y_valid_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM WITH ALL COURSE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results4, train_scores4, test_scores4, models4 = gridsearch_all_SMOTE(SVC(random_state=1, probability=True),svc_params_adj, \n",
    "                                          X_train_crs,y_train_crs,X_valid_crs,y_valid_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE WITH SELECTED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results5, train_scores5, test_scores5, models5 = gridsearch_all_SMOTE(DecisionTreeClassifier(random_state=1),tree_params_adj, \n",
    "                                          X_train_select,y_train_select,X_valid_select,y_valid_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE WITH COURSES ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results6, train_scores6, test_scores6, models6 = gridsearch_all_SMOTE(DecisionTreeClassifier(random_state=1),tree_params_adj, \n",
    "                                          X_train_crs,y_train_crs,X_valid_crs,y_valid_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE WITH ENROLLMENT ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results7, train_scores7, test_scores7, models7 = gridsearch_all_SMOTE(DecisionTreeClassifier(random_state=1),tree_params_adj, \n",
    "                                          X_train_enrl,y_train_enrl,X_valid_enrl,y_valid_enrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this function to get the test scores after hyper parameter tuning and feature selection\n",
    "def get_test_scores(model_dict, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_neg = []\n",
    "    f1_pos = []\n",
    "    f1_macro = []\n",
    "\n",
    "    all_scores = {}\n",
    "    all_probs = {}\n",
    "    all_preds = {}\n",
    "    \n",
    "    all_sig_feats = {}\n",
    "    \n",
    "    for file in file_names: \n",
    "\n",
    "        model_fit = model_dict[file].fit(X_train[file], y_train[file])\n",
    "        predictions = model_fit.predict(X_test[file])\n",
    "        all_preds[file] = predictions\n",
    "        \n",
    "        f1_labels = f1_score(y_test[file], predictions, zero_division=0, average=None)\n",
    "\n",
    "        f1_macro.append(f1_score(y_test[file], predictions, average='macro'))\n",
    "        f1_pos.append(f1_labels[1])\n",
    "        f1_neg.append(f1_labels[0])\n",
    "        precision.append(precision_score(y_test[file], predictions, pos_label='1'))\n",
    "        recall.append(recall_score(y_test[file], predictions, pos_label='1'))\n",
    "\n",
    "        \n",
    "        all_scores[file] = {'f1_pos': f1_labels[1],\n",
    "                            'f1_neg': f1_labels[0],\n",
    "                            'f1_macro': f1_score(y_test[file], predictions, average='macro'),\n",
    "                            'precision': precision_score(y_test[file], predictions,pos_label='1', zero_division=0), \n",
    "                            'recall': recall_score(y_test[file], predictions,pos_label='1', zero_division=0)}    \n",
    "\n",
    "        \n",
    "        ''' Collect Feature Importances (AdaBoost Only)'''\n",
    "        \n",
    "        #feature_importances = model_fit.named_steps[\"adaboostclassifier\"].feature_importances_\n",
    "        #feature_names = model_fit.named_steps[\"adaboostclassifier\"].feature_names_in_\n",
    "        \n",
    "        \n",
    "        all_sig_feats[file] = {'scores': feature_importances}#, 'names': feature_names}\n",
    "        \n",
    "        \n",
    "        '''ROC AUC'''\n",
    "        \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test[file],  predictions.astype(int), pos_label='1')\n",
    "        \n",
    "        auc = metrics.roc_auc_score(y_test[file].astype(int), predictions.astype(int))\n",
    "        print(file, auc)\n",
    "\n",
    "        \n",
    "    averages = {'f1_positive': statistics.mean(f1_pos), 'f1_negative': statistics.mean(f1_neg),\n",
    "                           'f1_macro': statistics.mean(f1_macro), 'precision': statistics.mean(precision), \n",
    "                            'recall' : statistics.mean(recall)}\n",
    "    \n",
    "    return all_scores, averages, all_preds, all_probs#, all_sig_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNING MODELS WITH TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_, avg_, preds_, probs_ = get_test_scores(models3, X_train_crs,y_train_crs, X_test_crs,y_test_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame.from_dict(score_, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COURSES ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1, avg_1, preds_1, probs_1 = get_test_scores(models5, X_train_crs,y_train_crs, X_test_crs,y_test_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1df = pd.DataFrame.from_dict(score_1, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2, avg_2, preds_2, probs_2 = get_test_scores(models5, X_train_crs,y_train_crs, X_test_crs,y_test_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2df = pd.DataFrame.from_dict(score_2, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_2df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_3, avg_3, preds_3, probs_3 = get_test_scores(models4, X_train_crs,y_train_crs, X_test_crs,y_test_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_3df = pd.DataFrame.from_dict(score_3, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_3df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENRL VARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_4, avg_4, preds_4, probs_4 = get_test_scores(models7, X_train_enrl,y_train_enrl, X_test_enrl,y_test_enrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_4df = pd.DataFrame.from_dict(score_4, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_4df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_5, avg_5, preds_5, probs_5, all_sign_feats5 = get_test_scores(models0, X_train_enrl,y_train_enrl, X_test_enrl,y_test_enrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_5df = pd.DataFrame.from_dict(score_5, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_5df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_6, avg_6, preds_6, probs_6 = get_test_scores(models2, X_train_enrl,y_train_enrl, X_test_enrl,y_test_enrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_6df = pd.DataFrame.from_dict(score_6, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_6df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECTED VARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABOOST SELECTED _ ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_71, avg_71, preds_71, probs_71 = get_test_scores(models11, X_train_select,y_train_select, X_test_select,y_test_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_71df = pd.DataFrame.from_dict(score_71, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_71df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Significant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize all features\n",
    "for file in file_names:\n",
    "    print(file)\n",
    "    for i in range(X_train_enrl[file].shape[1]):\n",
    "        if rfe_ft[file]['supp'][i] == True:   \n",
    "            print('Column: %d, Name: %s, Selected %s, Rank: %.3f' % (i, train_enrl_crs[file].columns[i],\n",
    "                                                             rfe_ft[file]['supp'][i], rfe_ft[file]['rank'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize all features\n",
    "for file in file_names:\n",
    "    #print(file)\n",
    "    for i in range(X_train_enrl[file].shape[1]):\n",
    "        if rfe_ft[file]['supp'][i] == True:   \n",
    "            print(file + \":\" +(train_enrl_crs[file].columns[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
